{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "data = pd.read_csv(f\"newdata_v1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# main hastags to be used\n",
    "main_hashtags = ['climatechange',\n",
    "                'climatecrisis',\n",
    "                'climateaction',\n",
    "                'saveearth',\n",
    "                'globalwarming',\n",
    "                'savetheplanet',\n",
    "                'climateemergency']\n",
    "\n",
    "hashtags = []\n",
    "groups_of_hashtags = [] # needed for #5 to create edges\n",
    "individual_hashtags = []\n",
    "for msg in data['Hashtags']:\n",
    "    hashtags_in_msg = []\n",
    "    for hashtag in msg.split():\n",
    "        tag = hashtag.strip('\\'.,[]').lower().replace('\\\\', '').replace('/', '').replace(':', '')\n",
    "        if \"â€¦\" not in tag and \"http\" not in tag:\n",
    "            hashtags.append(tag)\n",
    "            hashtags_in_msg.append(tag)\n",
    "            if tag not in individual_hashtags:\n",
    "                individual_hashtags.append(tag)\n",
    "    groups_of_hashtags.append(hashtags_in_msg)\n",
    "                \n",
    "# all individual usernumbers per main hashtag\n",
    "individual_users = dict.fromkeys(main_hashtags, [])\n",
    "\n",
    "# number of tweets per main hashtag\n",
    "num_of_tweets = dict.fromkeys(main_hashtags, 0)\n",
    "\n",
    "# count of individual users per main hashtag\n",
    "individual_users_count = dict.fromkeys(main_hashtags, 0)\n",
    "\n",
    "users = data['Username']\n",
    "\n",
    "for index, tweet_hashtags in enumerate(data['Hashtags']):\n",
    "    for hashtag in main_hashtags:\n",
    "        if hashtag in tweet_hashtags:\n",
    "            num_of_tweets[hashtag] += 1\n",
    "            if users[index] not in individual_users[hashtag]:\n",
    "                individual_users[hashtag].append(users[index])\n",
    "                individual_users_count[hashtag] += 1\n",
    "\n",
    "print(len(individual_hashtags))\n",
    "print(num_of_tweets)\n",
    "print(individual_users_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show number of tweets per hashtag\n",
    "plt.figure(figsize=(12,8))\n",
    "number_of_tweets = [key for key, val in num_of_tweets.items() for _ in range(val)]\n",
    "plt.hist(number_of_tweets, bins=7, color='green',edgecolor='black')\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.xlabel('Hashtag used')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#show number of users per hashtag\n",
    "plt.figure(figsize=(12,8))\n",
    "number_of_users = [key for key, val in individual_users_count.items() for _ in range(val)]\n",
    "plt.hist(number_of_users, bins=7, color='green',edgecolor='black')\n",
    "plt.ylabel('Number of individual users')\n",
    "plt.xlabel('Hashtag used')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# list of languages in tweets\n",
    "languages = []\n",
    "for tweet_lang in data['Language']:\n",
    "    if tweet_lang not in languages:\n",
    "        languages.append(tweet_lang)\n",
    "        \n",
    "# dict for number of tweets per language\n",
    "lang_count = dict.fromkeys(languages, 0)\n",
    "\n",
    "for tweet in data['Language']:\n",
    "    for lang in languages:\n",
    "        if lang in tweet:\n",
    "            lang_count[lang] += 1\n",
    "\n",
    "# sort dictionary by value\n",
    "lang_count = dict(sorted(lang_count.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# labels dict for showing 5 most popular languages\n",
    "# init 'other' to prevent keyerror\n",
    "labels = {'other': 0}\n",
    "\n",
    "for index, lang in enumerate(lang_count.keys()):\n",
    "    if index >= 4:\n",
    "        labels['other'] += lang_count[lang]\n",
    "    else:\n",
    "        labels[lang] = lang_count[lang]\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(labels.values(), labels=labels.keys(), autopct='%1.1f%%', startangle=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import reverse_geocoder as rg\n",
    "import pycountry\n",
    "\n",
    "#show geographic information of the tweets\n",
    "\n",
    "countries = {}\n",
    "\n",
    "for geodata in data['Geo']:\n",
    "    if type(geodata) != float:\n",
    "        geodata = ast.literal_eval(geodata)\n",
    "        if 'coordinates' in geodata:\n",
    "            latlong = (geodata['coordinates']['coordinates'][1], geodata['coordinates']['coordinates'][0])\n",
    "            location = rg.search(latlong)\n",
    "            country = pycountry.countries.get(alpha_2=location[0]['cc']).name\n",
    "            if country not in countries:\n",
    "                countries[country] = 1\n",
    "            else:\n",
    "                countries[country] += 1\n",
    "        \n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(countries.values(), labels=countries.keys(), autopct='%1.1f%%', startangle=90)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vader tool analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "vader_data = []\n",
    "for sentence in data['Text']:\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    vader_data.append(vs)\n",
    "    #print(\"{} {}\".format(index, vs))\n",
    "    \n",
    "fig = px.scatter_ternary(vader_data, a=\"neg\", b=\"neu\", c=\"pos\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct a social graph\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for hashtag in individual_hashtags:\n",
    "    G.add_node(hashtag)\n",
    "\n",
    "for group in groups_of_hashtags: # groups_of_hashtags is initialized at #1\n",
    "    for hashtag in group:\n",
    "        for other_hashtag in group:\n",
    "            if hashtag != other_hashtag:\n",
    "                G.add_edge(hashtag, other_hashtag)\n",
    "\n",
    "pos = nx.spring_layout(G, k=0.06, iterations=20)\n",
    "plt.figure(figsize=(256, 256))\n",
    "nx.draw(G, pos,node_size = 30, font_size = 15,with_labels=True,font_color='red')\n",
    "\n",
    "#get the top 10 in centrality for task 9      \n",
    "max_degrees = []\n",
    "for i in list(G.nodes):\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    currentmax = max(degree_centrality, key=degree_centrality.get)\n",
    "    max_degrees.append(currentmax)\n",
    "    G.remove_node(currentmax)\n",
    "    j += 1\n",
    "    if j == 10:\n",
    "        break\n",
    "        \n",
    "top10 = ['climatechange', 'climatecrisis', 'climateemergency', 'globalwarming', 'climateaction', 'savetheplanet', 'climate', 'climateactionnow', 'environment', 'sustainability']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate main global properties of the graph\n",
    "# number of nodes in variable\n",
    "num_nodes = G.number_of_nodes()\n",
    "# number of edges in variable \n",
    "num_edges = G.number_of_edges()\n",
    "# degree centrality in a variable\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "degree = nx.degree(G)\n",
    "\n",
    "# average degree centrality\n",
    "average_degree_centrality = sum(degree_centrality.values())/num_nodes\n",
    "\n",
    "try:\n",
    "        dia = nx.diameter(G)\n",
    "except nx.NetworkXError:\n",
    "        dia = \"infinite (not connected)\"\n",
    "\n",
    "clustering_coefficient = nx.average_clustering(G)\n",
    "\n",
    "# size of the largest component\n",
    "components = [x for x in nx.connected_components(G)]\n",
    "largest_component = max(components, key=len)\n",
    "\n",
    "graph_data = {'Number of nodes': num_nodes,\n",
    "              'Number of edges': num_edges,\n",
    "              'Average degree centrality': average_degree_centrality,\n",
    "              'Diameter': dia,\n",
    "              'Clustering coefficient': clustering_coefficient,\n",
    "              'Size of largest component': len(largest_component)}\n",
    "\n",
    "main_properties_dataframe = pd.DataFrame(graph_data, index=['Normal properties'])\n",
    "print(G.degree['climatechange'], G.degree['climatecrisis'], G.degree['globalwarming'],G.degree['climateemergency'])\n",
    "print(main_properties_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot clustering coefficient and degree\n",
    "local_clustering_sequence = sorted(nx.clustering(G).values(), reverse=True)\n",
    "degrees = [val for (node, val) in sorted(G.degree(), key=lambda pair: pair[0])]\n",
    "\n",
    "degrees = sorted(degrees)\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.hist(degrees, label=None, stacked=False, density=False, bins=100)\n",
    "ax1.set_title(\"Degree Distribution\")\n",
    "ax1.set_xlabel(\"Degree\")\n",
    "ax1.set_ylabel(\"Frequency\")\n",
    "ax1.set_xlim(0, 600)\n",
    "ax1.tick_params(axis='x', which='minor', bottom=True, labelbottom=False)\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.hist(local_clustering_sequence, bins=20)\n",
    "ax2.set_title(\"Local Clustering Coefficient Distribution\")\n",
    "ax2.set_xlabel(\"Local Clustering Coefficient\")\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label propagation algorithm for finding communities\n",
    "communities = nx.algorithms.community.label_propagation.label_propagation_communities(G)\n",
    "communities = [list(x) for x in communities]\n",
    "\n",
    "# graph of communities\n",
    "G_com = nx.Graph()\n",
    "        \n",
    "for community in communities:\n",
    "    G_com.add_nodes_from(community)\n",
    "    edges = [(a, b) for idx, a in enumerate(community) for b in community[idx + 1:]]\n",
    "    G_com.add_edges_from(edges)\n",
    "    \n",
    "\n",
    "# number of nodes in variable\n",
    "num_nodes_com = G_com.number_of_nodes()\n",
    "# number of edges in variable \n",
    "num_edges_com = G_com.number_of_edges()\n",
    "\n",
    "degree_centrality_com = nx.degree_centrality(G_com)\n",
    "degree_com = nx.degree(G_com)\n",
    "\n",
    "# average degree centrality\n",
    "average_degree_centrality_com = sum(degree_centrality_com.values())/num_nodes_com\n",
    "\n",
    "try:\n",
    "        dia_com = nx.diameter(G_com)\n",
    "except nx.NetworkXError:\n",
    "        dia_com = \"infinite (not connected)\"\n",
    "\n",
    "#clustering_coefficient_com = nx.average_clustering(G_com)\n",
    "clustering_coefficient_com = 0\n",
    "\n",
    "# size of the largest component\n",
    "components_com = [x for x in nx.connected_components(G_com)]\n",
    "largest_component_com = max(components_com, key=len)\n",
    "\n",
    "\n",
    "com_data = {'Number of nodes': num_nodes_com,\n",
    "        'Number of edges': num_edges_com,\n",
    "        'Average degree centrality': average_degree_centrality_com,\n",
    "        'Diameter': dia_com,\n",
    "        'Clustering coefficient': clustering_coefficient_com,\n",
    "        'Size of largest component': len(largest_component_com)}\n",
    "\n",
    "com_properties_dataframe = pd.DataFrame(com_data, index=['Community properties'])\n",
    "compare_properties_dataframe = pd.concat([main_properties_dataframe, com_properties_dataframe])\n",
    "print(compare_properties_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 9\n",
    "import tweepy\n",
    "import botometer\n",
    "\n",
    "bots = [207, 2130, 7]\n",
    "index = 2344\n",
    "j = 0\n",
    "\n",
    "#find tweets connected to the top 10 hashtags\n",
    "key_players_pos = []\n",
    "for i in groups_of_hashtags:\n",
    "    if 'climatechange' in i or 'climatecrisis' in i or 'climateemergency' in i or 'globalwarming' in i or 'climateaction' in i or 'savetheplanet' in i or 'climate' in i or 'climateactionnow' in i or 'environment' in i or 'sustainability' in i:\n",
    "        key_players_pos.append(j)\n",
    "    j += 1\n",
    "    \n",
    "#draw pie chart\n",
    "labels = 'bots', 'humans'\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(bots, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#Initialize rapid API\n",
    "rapidapi_key = \"xxxx\"\n",
    "twitter_app_auth = {\n",
    "    'consumer_key': 'xxxx',\n",
    "    'consumer_secret': 'xxxx',\n",
    "    'access_token': 'xxxx',\n",
    "    'access_token_secret': 'xxxx',\n",
    "  }\n",
    "bom = botometer.Botometer(wait_on_ratelimit=True,\n",
    "                          rapidapi_key=rapidapi_key,\n",
    "                          **twitter_app_auth\n",
    "\n",
    "#request data\n",
    "while True:\n",
    "    try:\n",
    "        ID = data['UserID'][index]\n",
    "        print(ID)\n",
    "        result = bom.check_account(ID)\n",
    "        if result[\"raw_scores\"][\"universal\"][\"overall\"] >= result[\"cap\"][\"universal\"]:\n",
    "            bots[0] += 1\n",
    "        else:\n",
    "            bots[1] += 1\n",
    "        index += 1\n",
    "    except tweepy.error.TweepError:\n",
    "        print(\"protected\")\n",
    "        index += 1\n",
    "        bots[2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task 10: rank tweets according to the number of likes and retweets\n",
    "tweet_score_rankings = []\n",
    "\n",
    "#initialize list for calculating rankings\n",
    "for tag in individual_hashtags:\n",
    "    tweet_score_rankings.append((tag, 0))\n",
    "    \n",
    "\n",
    "for index, tags in enumerate(data['Hashtags']):\n",
    "    #if retweet, dont calculate score since all the data transfers from the original tweet to retweets\n",
    "    if data['Text'][index].startswith(\"RT\"):\n",
    "        continue\n",
    "    \n",
    "    likes = np.sqrt(data['Likes'][index])\n",
    "    retweets = np.sqrt(data['Retweets'][index])\n",
    "    \n",
    "    final_score = likes+(retweets*3)\n",
    "    \n",
    "    #loop through the hashtags in this tweet\n",
    "    for tag in tags.split():\n",
    "        #clean the tag\n",
    "        tag = tag.strip('\\'.,[]').lower().replace('\\\\', '').replace('/', '').replace(':', '')\n",
    "        \n",
    "        #add new score to all hashtags that were included in this tweet\n",
    "        for j, one_tag in enumerate(tweet_score_rankings):\n",
    "            if tweet_score_rankings[j][0] == tag:\n",
    "                tweet_score_rankings[j] = (tweet_score_rankings[j][0], tweet_score_rankings[j][1]+final_score)\n",
    "\n",
    "#sort the ranking list\n",
    "sorted_rankings = sorted(tweet_score_rankings, key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "#50 best ranked tweets\n",
    "for i in range(50):\n",
    "    print(sorted_rankings[i])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e77af9e182af232d9d00794f4eaffd37193d897edd2cf9f189ddce53b8909cdc"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
